diff --git a/configs/hparams_search/cmapss_optuna.yaml b/configs/hparams_search/cmapss_optuna.yaml
index 61665af..f5c07c9 100644
--- a/configs/hparams_search/cmapss_optuna.yaml
+++ b/configs/hparams_search/cmapss_optuna.yaml
@@ -24,7 +24,7 @@ hydra:
     storage: sqlite:///hparams.db
 
     # name of the study to persist optimization results
-    study_name: transformer
+    study_name: paper
 
     # number of parallel workers
     n_jobs: 10
diff --git a/src/models/exp.py b/src/models/exp.py
index 0a3658d..9b18bd2 100644
--- a/src/models/exp.py
+++ b/src/models/exp.py
@@ -6,12 +6,21 @@ from torch.nn import TransformerEncoder, TransformerEncoderLayer
 class CNNLSTMTransformer(nn.Module):
     """Experimental network for multi variate time series foreacasting."""
 
-    def __init__(self, conv_out: int, lstm_hidden: int):
+    def __init__(
+        self,
+        nhead: int,
+        dim_feedforward: int,
+        num_encoder_layers: int,
+        lstm_hidden: int,
+        num_lstm_layers: int,
+    ):
         super().__init__()
-        encoder_layer = TransformerEncoderLayer(d_model=30, nhead=6, dim_feedforward=420, batch_first=True)
-        self.encoder = TransformerEncoder(encoder_layer, num_layers=6)
+        encoder_layer = TransformerEncoderLayer(
+            d_model=30, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True
+        )
+        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
 
-        self.lstm = nn.LSTM(30, lstm_hidden, 2, batch_first=True, dropout=0.2)
+        self.lstm = nn.LSTM(420, lstm_hidden, num_lstm_layers, batch_first=True, dropout=0.2)
         self.tanh = nn.Tanh()
 
         self.fc1 = nn.Linear(lstm_hidden, 16)
@@ -22,12 +31,10 @@ class CNNLSTMTransformer(nn.Module):
 
     def forward(self, x: torch.Tensor):
         x = self.encoder(x)
-        # print(x.shape)
 
-        # x = x.view(x.size(0), -1)
-        x = self.lstm(x)[0]
-        # print(x.shape)
-        x = self.tanh(x[:, -1, :])
+        x = x.view(x.size(0), -1)
+        x, _ = self.lstm(x)
+        x = self.tanh(x)
 
         x = self.fc1(x)
         x = self.relu(x)
diff --git a/tests/test_exp.py b/tests/test_exp.py
index 732214f..6e75287 100644
--- a/tests/test_exp.py
+++ b/tests/test_exp.py
@@ -10,8 +10,27 @@ def input():
     return torch.randn(32, 14, 30)
 
 
-def test_cnn_lstm_transformer(input: Tensor) -> None:
-    model = CNNLSTMTransformer(conv_out=32, lstm_hidden=50)
+@pytest.mark.parametrize("nhead", [6, 10])
+@pytest.mark.parametrize("dim_feedforward", [1024, 2048])
+@pytest.mark.parametrize("num_encoder_layers", [1, 2, 4])
+@pytest.mark.parametrize("lstm_hidden", [16, 32 ,50])
+@pytest.mark.parametrize("num_lstm_layers", [1, 2, 3])
+def test_cnn_lstm_transformer(
+    input: Tensor,
+    nhead: int,
+    dim_feedforward: int,
+    num_encoder_layers: int,
+    lstm_hidden: int,
+    num_lstm_layers: int,
+) -> None:
+    model = CNNLSTMTransformer(
+        nhead=nhead,
+        dim_feedforward=dim_feedforward,
+        num_encoder_layers=num_encoder_layers,
+        lstm_hidden=lstm_hidden,
+        num_lstm_layers=num_lstm_layers,
+    )
+
     output = model(input)
     assert model.encoder is not None
     assert isinstance(output, Tensor)
