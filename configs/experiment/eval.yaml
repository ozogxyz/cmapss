# @package _global_

# to execute this experiment run:
# python train.py experiment=fd1

defaults:
  - override /callbacks: default.yaml
  - override /datamodule: cmapss.yaml
  - override /model: transformer.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default
# configurations set above this allows you to overwrite only specified parameters
tags: ["optuna_sweep"]

seed: 42

# optuna best performance 12.23 RMSE
datamodule:
  max_rul: 1
  batch_size: 64
model:
  optimizer:
    lr: 0.003
    weight_decay: 0.002
  net:
    nhead: 3
    dim_feedforward: 512
    num_encoder_layers: 1
    lstm_hidden: 16
    num_lstm_layers: 1
callbacks:
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/rmse_best
    min_delta: 0.5
    patience: 3
    verbose: true
  model_summary:
    max_depth: 0
trainer:
  accelerator: cpu
  max_epochs: 10
extras:
  print_config: true
