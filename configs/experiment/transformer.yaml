# @package _global_

# to execute this experiment run:
# python train.py experiment=fd1

defaults:
  - override /callbacks: default.yaml
  - override /datamodule: cmapss.yaml
  - override /model: transformer.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default
# configurations set above this allows you to overwrite only specified parameters
tags: ["optuna_sweep"]

seed: 42

# optuna best performance 11.49 RMSE
datamodule:
  batch_size: 64
model:
  optimizer:
    lr: 0.003
    weight_decay: 0.002
  net:
    nhead: 3
    dim_feedforward: 512
    num_encoder_layers: 1
    lstm_hidden: 16
    num_lstm_layers: 1
    dropout: 0.1
callbacks:
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/rmse_best
    min_delta: 0.5
    patience: 1
    verbose: true
  model_summary:
    max_depth: 0
trainer:
  accelerator: cpu
  max_epochs: 10
extras:
  print_config: false
# logger:
#   wandb:
#     _target_: pytorch_lightning.loggers.wandb.WandbLogger
#     save_dir: ${paths.log_dir}
#     project: cmapss-transformer
#     prefix: ""
#     group: cmapss
#     tags:
#       - transformer
hparams_search:
  transformer_optuna:
    hydra:
      sweeper:
        study_name: transformer_encoder